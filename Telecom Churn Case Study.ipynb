{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement ###\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.o reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    "**Churn Phases**\n",
    "- In ‘good’ phase the customer is happy with the service and behaves as usual\n",
    "- In ‘action’ phase The customer experience starts to sore in this phase\n",
    "- In ‘churn’ phase the customer is said to have churned\n",
    "\n",
    "#### Business Goal ####\n",
    "\n",
    "In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    "#### Outcomes ####\n",
    "\n",
    "- Predict churn only on high-value customers\n",
    "- Predict usage-based definition to define churn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('float_format', '{:.2f}'.format)\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = pd.read_csv('telecom_churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the head of our master dataset\n",
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dimensions of the dataframe\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the statistical aspects of the dataframe\n",
    "telecom_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the type of each column\n",
    "telecom_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below columns are breaking the convention. So we will rename them appropriately\n",
    "vbc_cols = [col for col in telecom_df.columns if 'vbc' in col]\n",
    "print(vbc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.rename(columns = {'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g': 'vbc_3g_8', 'sep_vbc_3g': 'vbc_3g_9'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataType Correction \n",
    "object_df = telecom_df.select_dtypes(include='object')\n",
    "object_df.head()\n",
    "\n",
    "#Looks like all are datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert object to date time\n",
    "\n",
    "for col in object_df.columns:\n",
    "    telecom_df[col] = pd.to_datetime(telecom_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to cleanse the data and create a Manageable Data Set for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which returns the columns with missing values > the cutoff percentage\n",
    "# Argument: cutoff percentage between 1 - 100\n",
    "def calculate_missing_values(data, cutoff):\n",
    "    missing_percent= round(data.isna().sum() / len(data.index) * 100)\n",
    "    print(\"{} features having more than {}% missing values:\".format(len(missing_percent[missing_percent > cutoff]), cutoff))\n",
    "    return missing_percent[missing_percent > cutoff]\n",
    "\n",
    "#Function to handle missing values across months \n",
    "# Argument: Take list of column names without month number suffix \n",
    "def impute_zero_in_missing(data, columnList):\n",
    "    for feature in [col + suffix for suffix in ['_6','_7','_8','_9'] for col in columnList]:\n",
    "        data[feature].fillna(0, inplace=True)\n",
    "        \n",
    "\n",
    "#Function to drop columns across months\n",
    "def drop_columns(data,columnList):\n",
    "    for feature in [col+suffix for suffix in ['_6','_7','_8','_9'] for col in columnList]:\n",
    "        data.drop([feature],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns which have only null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df[telecom_df.isnull().all(axis=1)]\n",
    "\n",
    "##Looks like there are no columns with only null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns with 1 unique value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cols = telecom_df.nunique()\n",
    "unique_cols[unique_cols == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.drop(unique_cols[unique_cols == 1].index, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with missing values greater than 70% or impute the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missing_values(telecom_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute Missing Value as 0 for all the **Recharge, Revenue, Night Pack, Fb User** Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValueColumnList=['total_rech_data', 'max_rech_data', 'count_rech_2g', 'count_rech_3g', 'av_rech_amt_data'\n",
    "                       , 'arpu_3g', 'arpu_2g', 'night_pck_user', 'fb_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the minimum value is 1, we are going to handle NA values by imputing with 0 \n",
    "# which means we are assuming there were no recharges done by the customer\n",
    "impute_zero_in_missing(telecom_df, missingValueColumnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missing_values(telecom_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missing_values(telecom_df, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop all features which have more than 70% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns(telecom_df,['date_of_last_rech_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missing_values(telecom_df, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns except date columns are for the month 9 i.e the Churn phase. \n",
    "This data will be eventually dropped when we tag churn/no churn, \n",
    "hence we will be skipping imputing these fields related to month 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation ###\n",
    "\n",
    "**Filter in High Value Customers which are the target of our analysis**\n",
    "\n",
    "* A high-value customers is defined as follows:\n",
    "\n",
    "- Those who have recharged with an amount more than or equal to X, where X is greater than 70th percentile of the average recharge amount in the first two months (the good phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column total recharge amount for data for aiding in finding High Value customer \n",
    "\n",
    "telecom_df['total_rech_amt_data_6'] = telecom_df.av_rech_amt_data_6 * telecom_df.total_rech_data_6\n",
    "telecom_df['total_rech_amt_data_7'] = telecom_df.av_rech_amt_data_7 * telecom_df.total_rech_data_7\n",
    "telecom_df['total_rech_amt_data_8'] = telecom_df.av_rech_amt_data_8 * telecom_df.total_rech_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create column for holding average of total recharge amount for good phase (the months June (6) and July (7))\n",
    "# We add the total recharge amount of call and data and find the average across two months\n",
    "telecom_df['total_avg_rech_amnt_Good_Phase'] = (telecom_df.total_rech_amt_6 + telecom_df.total_rech_amt_data_6 \\\n",
    "                                               + telecom_df.total_rech_amt_7+ telecom_df.total_rech_amt_data_7)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter values greater than 70th percentile of total average recharge amount for good phase \n",
    "seventieth_percentile = telecom_df.total_avg_rech_amnt_Good_Phase.quantile(0.7)\n",
    "\n",
    "telecom_df_high_val_cust = telecom_df[telecom_df.total_avg_rech_amnt_Good_Phase > seventieth_percentile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"70th Percentile of Average Recharge amount in Good Phase (June and July month) is \", seventieth_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Target Variable Churn based on the following criteria**\n",
    "\n",
    "* A Churned Customer is defined as follows : \n",
    "* A Customer has churned (churn=1, else 0) if in the Ninth Month he/she has not made any calls (either incoming or outgoing) \n",
    "* AND have not used mobile internet even once. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column \"churn\", values would be either 1 (churn) or 0 (non-churn)\n",
    "telecom_df_high_val_cust['churn'] = \\\n",
    "        np.where(telecom_df_high_val_cust[['total_ic_mou_9','total_og_mou_9', \\\n",
    "                                           'vol_2g_mb_9', 'vol_3g_mb_9']].sum(axis=1) == 0, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the % of churn/non churn customers\n",
    "churn_percentage = telecom_df_high_val_cust.churn.value_counts(normalize=True)\n",
    "print(churn_percentage)\n",
    "churn_percentage.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : The churn percentage is around 8%. This indicates that there is a slight imbalance in the dataset which will need to be corrected in modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop all data of the ninth Month as that is our Target Variable**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_month_columns =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('_9')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_month_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns corresponding to the churn phase\n",
    "telecom_df_high_val_cust.drop(churn_month_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : EDA ### \n",
    "\n",
    "We will now perform EDA and try to get insights into the data. \n",
    "Based on the insights we could define our approach to training, remove outliers, remove highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CheckPoint 1 \n",
    "telecom_eda_df = telecom_df_high_val_cust.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Bi variate Analysis of Various Variables with the Churn Variable ####\n",
    "We will draw trends of the selected categorical variables data wrt to the Label, Churn and see if we can derive any meaningful insights\n",
    "\n",
    "**Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create box plot for  6th, 7th and 8th month\n",
    "def create_box_plot(column):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    df = telecom_eda_df\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.boxplot(data=df, y=column+\"_6\",x=\"churn\", showfliers=False)\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.boxplot(data=df, y=column+\"_7\",x=\"churn\", showfliers=False)\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.boxplot(data=df, y=column+\"_8\",x=\"churn\", showfliers=False)\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create box plot for  6th, 7th and 8th month\n",
    "def create_bar_plot(column):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    df = telecom_eda_df\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.barplot(data=df, y=column+\"_6\",x=\"churn\")\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.barplot(data=df, y=column+\"_7\",x=\"churn\")\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.barplot(data=df, y=column+\"_8\",x=\"churn\")\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showbarlabel(graph, rotate=0):\n",
    "    graph.set_xticklabels(graph.get_xticklabels(),rotation=rotate)\n",
    "    for p in graph.patches:\n",
    "        height = p.get_height()\n",
    "        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(by,rotate=0):\n",
    "    df = telecom_df_high_val_cust\n",
    "    graph = df[by].value_counts(sort=True).plot(kind='bar')\n",
    "    showbarlabel(graph,rotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for Total recharge amount :\n",
    "create_box_plot('total_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Total Recharge amount drops in month 8 indicating Churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for maximum recharge amount :\n",
    "create_box_plot('max_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**  : We can see that there is a huge drop in maximum recharge amount for churned customers in the 8th month i.e action phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot('av_rech_amt_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Average Recharge amount drops in month 8 indicating Churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for total recharge amount data :\n",
    "create_box_plot('total_rech_amt_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : We can see a drop in the total recharge for data for churned customers in the 8th Month i.e Action Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Recharge Number related column list\n",
    "recharge_num_columns = [col for col in telecom_df_high_val_cust if 'rech_num' in col.lower()]\n",
    "recharge_num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*telecom_df_high_val_cust[recharge_num_columns].isnull().sum()/len(telecom_df_high_val_cust.index)\n",
    "# We don't have any missing values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for total recharge number:\n",
    "create_box_plot('total_rech_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : We can see that there is a huge drop in total recharge number for churned customers in the 8th month i.e action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Recharge data related column list\n",
    "recharge_data_columns = [col for col in telecom_df_high_val_cust if 'rech_data' in col.lower()]\n",
    "recharge_data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*telecom_df_high_val_cust[recharge_data_columns].isnull().sum()/len(telecom_df_high_val_cust.index)\n",
    "# We don't have any missing values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for total recharge data:\n",
    "create_box_plot('total_rech_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Again we can see that there is a huge drop in total recharge amount data for churned customers in the 8th month i.e action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for max recharge for data:\n",
    "create_box_plot('max_rech_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : There is a huge drop in max recharge amount data for churned customers in the 8th month i.e action phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for Last  day recharge amount  :\n",
    "create_box_plot('last_day_rch_amt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for volume of 2G and 3G usage columns:\n",
    "create_box_plot('vol_2g_mb')\n",
    "create_box_plot('vol_3g_mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see 2g and 3g usage for churned customers drops in the 8th month i.e Action phase.\n",
    "\n",
    "However in general we see the usage is low for churned customer across months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for count of 2G and 3G recharge columns:\n",
    "create_bar_plot('count_rech_2g')\n",
    "create_bar_plot('count_rech_3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see 2g and 3g recharge counts for churned customers drops in the 8th month i.e Action phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for arpu of 2G and 3G usage columns:\n",
    "create_box_plot('arpu_2g')\n",
    "create_box_plot('arpu_3g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting bar plot for arpu of 2G and 3G usage columns:\n",
    "create_bar_plot('arpu_2g')\n",
    "create_bar_plot('arpu_3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see 2g and 3g arpu for churned customers drops in the 8th month i.e Action phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for monthly subcription of 2G and 3G usage columns:\n",
    "create_box_plot('monthly_2g')\n",
    "create_box_plot('monthly_3g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot as box plot doesn't show any pattern\n",
    "create_bar_plot('monthly_2g')\n",
    "create_bar_plot('monthly_3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see 2g and 3g monthly subscription for churned customers drops in the 8th month i.e Action phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for small duration subscription of 2g and 3g data\n",
    "# Plotting a bar plot as box plot doesn't show any pattern\n",
    "create_bar_plot('sachet_2g')\n",
    "create_bar_plot('sachet_3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see 2g and 3g small duration subscription for churned customers drops in the 8th month i.e Action phase.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting volume based 3g usage\n",
    "create_bar_plot('vbc_3g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the  day  columns\n",
    "day_columns = [col for col in telecom_df_high_val_cust if 'day' in col.lower()]\n",
    "day_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*telecom_df_high_val_cust[day_columns].isnull().sum()/len(telecom_df_high_val_cust.index)\n",
    "# We don't have any missing values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot('last_day_rch_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge drop in 8th month for last day recharge amount indicating  churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Date column list\n",
    "date_columns = [col for col in telecom_df_high_val_cust if 'date' in col.lower()]\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*telecom_df_high_val_cust[date_columns].isnull().sum()/len(telecom_df_high_val_cust.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing value indicates that recharge date and the recharge value are missing together which means the customer didn't recharge for that month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust[telecom_df_high_val_cust.date_of_last_rech_6.isnull()][['total_rech_data_6','date_of_last_rech_6']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for ARPU \n",
    "create_box_plot('arpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** \n",
    "We see the ARPU for churned customers drops in the 8th month i.e Action phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Onnet Minutes of Usage\n",
    "create_box_plot('onnet_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls on service provider network drops in month 8 indicates Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Offnet Minutes of Usage\n",
    "create_box_plot('offnet_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls to different network drops in month 8 indicates Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Observation from above EDA #### \n",
    "- We see churned customers drops in the 8th month i.e Action phase.\n",
    "- We can also see that the trend reversal happens drastically in the 9th month. \n",
    "- In the 6th and 7th month, the variance captured between the Churned and Non Churned customers is not much.\n",
    "- So to reduce dimensionality it might be a good idea to average out the values of both the columns and drop the individual months. \n",
    "- We can test this hypothesis in the models later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Dimensionality Reduction  ####\n",
    "\n",
    "Based  on the EDA done above, we will reduce the dimensionality to make forecasting of the trends easier based on the  features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction | Drop Highly Correlated Columns as a PreRequisite to EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a copy to avoid regenerating master for each iterations\n",
    "##Check point 2\n",
    "training_df = telecom_eda_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Highly correlated data and drop Highly Correlated Columns\n",
    "cor = training_df.corr()\n",
    "cor.loc[:,:] = np.tril(cor, k=-1)\n",
    "sns.heatmap(cor, cmap='Greens', annot=False, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are strong multicollinearity issues, Lets drop data which is multi collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = training_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.85\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter columns of months where only all months are highly correlated, others we will ignore \n",
    "to_drop_filtered = ['std_og_t2t_mou_6',\n",
    " 'std_og_t2t_mou_7',\n",
    " 'std_og_t2t_mou_8',\n",
    " #'std_og_t2m_mou_7',\n",
    " #'std_og_t2m_mou_8',\n",
    "# 'isd_og_mou_7',\n",
    "# 'isd_og_mou_8',\n",
    " #'total_og_mou_8',\n",
    " 'total_ic_mou_6',\n",
    " 'total_ic_mou_7',\n",
    " 'total_ic_mou_8',\n",
    " 'total_rech_amt_6',\n",
    " 'total_rech_amt_7',\n",
    " 'total_rech_amt_8',\n",
    " 'count_rech_2g_6',\n",
    " 'count_rech_2g_7',\n",
    " 'count_rech_2g_8',\n",
    " 'arpu_2g_6',\n",
    " 'arpu_2g_7',\n",
    " 'arpu_2g_8',\n",
    " 'sachet_2g_6',\n",
    " 'sachet_2g_7',\n",
    " 'sachet_2g_8',\n",
    "# 'monthly_3g_7',\n",
    "#'monthly_3g_8',\n",
    " 'sachet_3g_6',\n",
    " 'sachet_3g_7',\n",
    " 'sachet_3g_8',\n",
    "]\n",
    "\n",
    "training_df.drop(to_drop_filtered, axis =1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction | Drop the Date Columns as they are already factored in the other columns, Drop Mobile Number as its not meaningful to prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop(['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','mobile_number'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Highly correlated data and drop Highly Correlated Columns\n",
    "cor = training_df.corr()\n",
    "cor.loc[:,:] = np.tril(cor, k=-1)\n",
    "sns.heatmap(cor, cmap='Greens', annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint 3\n",
    "# We can run from checkpoints rather than run the entire notebook for validation\n",
    "telecom_df_high_val_cust = training_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction | Avg Out the 6th and 7th Month to reduce the number of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Avg Column \n",
    "\n",
    "col_list = telecom_df_high_val_cust.filter(regex='_6|_7').columns.str[:-2]\n",
    "col_list.unique()\n",
    "\n",
    "print (telecom_df_high_val_cust.shape)\n",
    "\n",
    "for idx, col in enumerate(col_list.unique()):\n",
    "    print(col)\n",
    "    avg_col_name = \"avg_\"+col+\"_av67\"\n",
    "    col_6 = col+\"_6\"\n",
    "    col_7 = col+\"_7\"\n",
    "    telecom_df_high_val_cust[avg_col_name] = (telecom_df_high_val_cust[col_6]  + telecom_df_high_val_cust[col_7])/ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the individual columns\n",
    "\n",
    "print (telecom_df_high_val_cust.shape)\n",
    "\n",
    "col_list = telecom_df_high_val_cust.filter(regex='_6|_7').columns\n",
    "\n",
    "telecom_df_high_val_cust.drop(col_list, axis=1, inplace=True)\n",
    "telecom_df_high_val_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis \n",
    "\n",
    "**Analyse sample of  categorical variables and see if we can draw any meaningful insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    counter = 0;\n",
    "    for i in range(min(nCol,nGraphShown)):\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), pd._libs.tslibs.timestamps.Timestamp)):\n",
    "            plt.subplot(nGraphRow, nGraphPerRow, counter+1)\n",
    "            #increment the counter\n",
    "            counter +=1\n",
    "            if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "                valueCounts = columnDf.value_counts()\n",
    "                valueCounts.plot.bar()\n",
    "            else:\n",
    "                columnDf.hist()\n",
    "                plt.ylabel('counts')\n",
    "                plt.xticks(rotation = 90)\n",
    "                plt.title(f'{columnNames[i]} (column {i})')\n",
    "        \n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(telecom_df_high_val_cust,30,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** -\n",
    "* Histogram distribution shows twin peaks(bimodal) or single peaks on the lower bars of the distribution.\n",
    "* The distribution is not normal, which indicates this could be a higher order linear or non linear distribution. \n",
    "* Smoothening may be required for better prediction. \n",
    "* The distribution is sparse or spread in one bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of sample of  Numeric/Continuous Features to see if we can derive any trends**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScatterMatrix(telecom_df_high_val_cust, 25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** :\n",
    "* Relation between the variables are not obviously linear\n",
    "* All numeric variables are having distributions which are not normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Tenure with Churn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert AON in Months\n",
    "telecom_df_high_val_cust['aon_mon'] = telecom_df_high_val_cust['aon']/30\n",
    "telecom_df_high_val_cust.drop('aon', axis=1, inplace=True)\n",
    "telecom_df_high_val_cust['aon_mon'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust.aon_mon.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(telecom_df_high_val_cust['aon_mon'], hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "ax.set_ylabel('No of Customers')\n",
    "ax.set_xlabel('Tenure (months)')\n",
    "ax.set_title('Customers by their tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_range = [0, 6, 12, 24, 60, 61]\n",
    "tn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "telecom_df_high_val_cust['tenure_range'] = pd.cut(telecom_df_high_val_cust['aon_mon'], tn_range, labels=tn_label)\n",
    "telecom_df_high_val_cust['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "temp = pd.Series(data = 'tenure_range')\n",
    "fig, ax = plt.subplots()\n",
    "width = len(telecom_df_high_val_cust['tenure_range'].unique()) + 6 + 4*len(temp.unique())\n",
    "fig.set_size_inches(width , 7)\n",
    "\n",
    "total = float(len(telecom_df_high_val_cust.index))\n",
    "ax = sns.countplot(x=\"tenure_range\", data=telecom_df_high_val_cust, palette=\"Set2\", hue = \"churn\");\n",
    "for p in ax.patches:\n",
    "                ax.annotate('{:1.1f}%'.format((p.get_height()*100)/float(len(telecom_df_high_val_cust))), (p.get_x()+0.05, p.get_height()+20))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation of  Churn with all Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Correlation of \"Churn\" with other variables:\n",
    "plt.figure(figsize=(20,10))\n",
    "telecom_df_high_val_cust.corr()['churn'].sort_values(ascending = False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "*Positive Corelation with*\n",
    "- Churn has got maximum correlation with \"Avg Std Outgoing Month 6 and 7\"\n",
    "- Roaming Outgoing/Incoming Minutes\n",
    "\n",
    "*Negative Corelation with*\n",
    "- fb Usage Plan for 8th Month\n",
    "- arpu 8th Month\n",
    "- Total Outgoing 8th Month\n",
    "\n",
    "Lets also validate if ARPU8 and TotalRech8 are correlated, and if so we should eliminate one of these variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relation between ARPU8 and TotalRecharge Num in 8th Month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n",
    "                                                              y='arpu_8')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Looks like they are not correlated and we will keep both the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Building\n",
    "\n",
    "Moving towards Model Building\n",
    "Also drop a **Mobile Number** and **Tenure**\n",
    "\n",
    "\n",
    "### 3.1 Pre Training Steps ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint 5\n",
    "\n",
    "model_df = telecom_df_high_val_cust[:].copy()\n",
    "telecom_df_high_val_cust.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping tenure_range since we have AON MONTH already and columns are highly coorelated\n",
    "model_df.drop('tenure_range', axis=1, inplace=True)\n",
    "\n",
    "#dropping total_avg_rech_amnt_Good_Phase which was calculated to find High Value customers.\n",
    "\n",
    "model_df.drop('total_avg_rech_amnt_Good_Phase', axis=1, inplace=True)\n",
    "\n",
    "#Since All The Values are realted to Price/ Cost/ Amount, Filling NaN with 0\n",
    "\n",
    "model_df.fillna(0, inplace=True)\n",
    "\n",
    "model_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating response and predictor variables\n",
    "response='churn'\n",
    "predictor=model_df.columns[model_df.columns != 'churn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Train and Test Sets\n",
    "\n",
    "In cases when classification problems  can exhibit a large imbalance in the distribution of the target classes,  it is recommended to use stratified sampling to ensure that relative class frequencies is approximately preserved in each train and validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(model_df, test_size=0.3, random_state=0,stratify=model_df[response])\n",
    "\n",
    "#Rows and columns after split\n",
    "print (df_train.shape)\n",
    "print (df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale all the predictor variables, so that there is equal weightage to all the features, and convergence is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_train[predictor] = scaler.fit_transform(df_train[predictor])\n",
    "df_test[predictor] = scaler.transform(df_test[predictor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train\n",
    "X_train = df_train.drop('churn', axis=1)\n",
    "y_train = df_train['churn']\n",
    "\n",
    "# X_test, y_test\n",
    "X_test = df_test.drop('churn', axis=1)\n",
    "y_test = df_test['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying SMOTE\n",
    "#We wont be applying Smote on the Test Data Set\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "try:\n",
    "    print(imblearn.__version__)\n",
    "except:\n",
    "    print(\"Please install SMOTE Package First\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling=SMOTE(random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def perform_pca_plot_scree_plot(X):\n",
    "    pca = PCA(random_state=0)\n",
    "    pca.fit(X)\n",
    "    fig = plt.figure(figsize = (8,6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative  variance explained')\n",
    "    plt.show()\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = perform_pca_plot_scree_plot(df_train[predictor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "Looks like 60 components are enough to describe 95% of the variance in the dataset.We'll choose 60 components for our modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(df_train[predictor].columns)\n",
    "df_pca = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':col})\n",
    "df_pca.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training ###\n",
    "\n",
    "*Approach*\n",
    "- We will first train a Logistical Regression Model, which will form our Baseline\n",
    "- We will then try  to try a Kernel and Random Forest and try to improve on our baseline\n",
    "- Kernel Model we will use PCA and optimise for Prediction \n",
    "- Random Forest Model we will optimise Forecasting and use it for find the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the confusion matrix.\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For y_true and y_pred display the Classification Metrics and plot Confusion Matrix\n",
    "def computeClassificationMetrics(y_test,y_test_pred, plot=True):\n",
    "        print('*'*20+\"Classification Report\"+'*'*20)\n",
    "        print(classification_report(y_test,y_test_pred))\n",
    "        \n",
    "        if plot:\n",
    "            # Compute confusion matrix\n",
    "            cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "            print('*'*20+'Confusion Matrix'+'*'*20)\n",
    "            # Plot non-normalized confusion matrix\n",
    "            class_names = [0,1]\n",
    "            plt.figure(figsize=(7,5))\n",
    "            plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names)                      \n",
    "            plt.show()\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for training models and testing on a test set. if plot is True, it plots a PRC curve for \n",
    "##training and test sets and finds the threshold where (precision*recall) is maximum.\n",
    "def model_fit(alg, X_train_data, y_train_data, X_test_data, y_test_data, plot=True):\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train_data, y_train_data)\n",
    "        \n",
    "    #Predict training set:\n",
    "    y_train_pred = alg.predict(X_train_data)\n",
    "    y_test_pred = alg.predict(X_test_data)\n",
    "      \n",
    "    computeClassificationMetrics(y_test, y_test_pred, plot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For CV reusults plot the  C Value vs Accuracy Results        \n",
    "def display_svm_stats(cv_results,param_value):\n",
    "    gamma = cv_results[cv_results['param_gamma']==param_value]\n",
    "    plt.plot(gamma[\"param_C\"], gamma[\"mean_test_score\"])\n",
    "    plt.plot(gamma[\"param_C\"], gamma[\"mean_train_score\"])\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Gamma=\"+str(param_value))\n",
    "    plt.ylim([0.60, 1])\n",
    "    plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n",
    "    plt.xscale('log') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Balancing by over-sampling minority class using SMOTE ####\n",
    "\n",
    "We will start by balancing the data set using SMOTE technique. First we will balance the data set by SMOTE and then apply logistic regression model on the balanced data set.\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "\n",
    "- SMOTE technique should be applied only on the training data, after splitting the data set into training set and validation set. \n",
    "- If we apply SMOTE before splitting the data, we would leak the information in validation set into the training set. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalance before SMOTE\n",
    "print(\"X_train Shape : \", X_train.shape)\n",
    "print(\"X_test Shape : \", X_test.shape)\n",
    "\n",
    "y_train_imb = (y_train != 0).sum()/(y_train == 0).sum()\n",
    "y_test_imb = (y_test != 0).sum()/(y_test == 0).sum()\n",
    "print(\"Imbalance in Train Data : \", y_train_imb)\n",
    "print(\"Imbalance in Test Data : \", y_test_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train dataset before SMOTE \", df_train[predictor].shape)\n",
    "x_tr,y_tr = sampling.fit_sample(df_train[predictor],df_train[response])\n",
    "print(\"Shape of train dataset after SMOTE : \", x_tr.shape)\n",
    "\n",
    "# Applying PCA : Principal Component Analysis using 60 components\n",
    "pca = IncrementalPCA(n_components=60)    \n",
    "X_train_pca = pca.fit_transform(x_tr)\n",
    "X_test_pca = pca.transform(df_test[predictor])\n",
    "print(\"Shape of train dataset after PCA : \", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_tr Shape\", x_tr.shape)\n",
    "print(\"y_tr Shape\", y_tr.shape)\n",
    "\n",
    "imb = (y_tr != 0).sum()/(y_tr == 0).sum()\n",
    "print(\"Imbalance in Train Data : \",imb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logistic Regression \n",
    "\n",
    "\n",
    "We will training a logistic regression and test it on a validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Logistric Regression\n",
    "logreg1 = LogisticRegression(random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call model fit and evalute the validation set\n",
    "model_fit(logreg1,X_train_pca, y_tr, X_test_pca, df_test[response] ,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We acheived a Accuracy of 83% an F1 Score of 0.86. This is a very decent prediction power on a unbalanced data set using Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Iteration 1: Using SVM with Appropriate Kernel for Predictive Model ####\n",
    "\n",
    "- Choose the right kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rbf Kernel\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.svm = SVC(kernel='rbf') \n",
    "\n",
    "# Call model fit and evalute the validation set\n",
    "model_fit(lr.svm ,x_tr, y_tr, df_test[predictor],df_test[response] ,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poly Kernel\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "lr.svm = SVC(kernel='poly') \n",
    "model_fit(lr.svm ,x_tr, y_tr, df_test[predictor],df_test[response] ,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Kernel\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "lr.svm = SVC(kernel='linear') \n",
    "model_fit(lr.svm ,x_tr, y_tr, df_test[predictor],df_test[response] ,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** : \n",
    "Since the Test set is unbalanced we need to look at the weighted score, for precision and Recall. \n",
    "Looks like the **Poly** Kernel is giving the 89% accuracy score with an F1 value of 0.91. This is really good. \n",
    "This also is in line with our understanding from EDA. \n",
    "\n",
    "First we will try to combine this with PCA and see if we can capture the variance in lower dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA to Kernel\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "lr.svm = SVC(kernel='poly') \n",
    "model_fit(lr.svm,X_train_pca, y_tr, X_test_pca, df_test[response] ,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** : Its very clear that PCA degrades the SVM Score. \n",
    "- The reason could be that PCA projects the values in the linear space and that is causing some information to be lost, in such a way that the SVM Kernel cannot read it.\n",
    "- https://www.researchgate.net/post/Is_there_a_specific_reason_that_using_PCA_gives_worse_results_than_without_using_it_in_SVM_classification\n",
    "- We will NOT be using PCA for SVM. \n",
    "- Lets next try to optimise the C and gamma hyperparameters for the SVM and see if we can improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # commenting as grid search is taking a long time on local workstation\n",
    "# # creating a KFold object with 5 splits \n",
    "# folds = KFold(n_splits = 3, shuffle = True, random_state = 101)\n",
    "\n",
    "# # Set the parameters by cross-validation\n",
    "# hyper_params = [ {'gamma': [1e-1,1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n",
    "# #hyper_params = [ {'gamma': [1e-1], 'C': [1]}]\n",
    "\n",
    "# # specify model\n",
    "# model = SVC(kernel=\"poly\")\n",
    "\n",
    "# # set up GridSearchCV()\n",
    "# model_cv_svm = RandomizedSearchCV(estimator = model, \n",
    "#                         param_distributions = hyper_params, \n",
    "#                         scoring= 'accuracy', \n",
    "#                         cv = folds,\n",
    "#                         n_jobs = -1,\n",
    "#                         verbose = 2,\n",
    "#                         return_train_score=True, random_state=100)      \n",
    "\n",
    "# # fit the model\n",
    "# model_cv_svm.fit(X_tr, y_tr)  \n",
    "\n",
    "\n",
    "# ## This will take some time, and you should twiddle your thumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting as grid search is taking a long time\n",
    "# # cv results\n",
    "# svm_cv_results = pd.DataFrame(model_cv_svm.cv_results_)\n",
    "# svm_cv_results['param_C'] = svm_cv_results['param_C'].astype('int')\n",
    "# gamma=[1e-1,1e-2, 1e-3, 1e-4]\n",
    "# plt.figure(figsize=(16,5))\n",
    "# plt.subplot(141)\n",
    "# display_svm_stats(svm_cv_results,gamma[0])\n",
    "# plt.subplot(142)\n",
    "# display_svm_stats(svm_cv_results,gamma[1])\n",
    "# plt.subplot(143)\n",
    "# display_svm_stats(svm_cv_results,gamma[2])\n",
    "# plt.subplot(144)\n",
    "# display_svm_stats(svm_cv_results,gamma[3])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "We wil be using the default values, as GridSearch is a brute force algorithm, and is taking long time to process on desktop workstation. \n",
    "Also with the poly kernel we are getting 89% Accuracy with Fscore of 0.91 which is better than our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # printing the optimal accuracy score and hyperparameters\n",
    "# best_score = model_cv_svm.best_score_\n",
    "# best_hyperparams = model_cv_svm.best_params_\n",
    "\n",
    "# log(\"The best test score is {0} corresponding to hyperparameters {1}\".format(round(best_score,2), best_hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "We wil be using the default values, as GridSearch is a brute force algorithm, and is taking long time to process on desktop workstation. \n",
    "Also with the poly kernel we are getting 89% Accurity with Fscore of 90 which is better than our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store session so that heavy computation results can be persisted in memory\n",
    "#import dill\n",
    "#dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Model Iteration 2: Using RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model fit and evalute the validation set\n",
    "model_fit(rfc,X_train_pca, y_tr, X_test_pca, df_test[response] ,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline Random Forest regression model gave a Recall of 92% and Precision of 92% on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning using GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter(parameters,x_train,y_train,n_folds = 5,max_depth=0):\n",
    "    \n",
    "    if(max_depth==0):\n",
    "        rf = RandomForestClassifier(random_state=0)\n",
    "    else :\n",
    "        rf = RandomForestClassifier(max_depth=max_depth, random_state=0)\n",
    "        \n",
    "    rf = GridSearchCV(rf, parameters, cv=n_folds,n_jobs = -1, scoring=\"accuracy\",return_train_score=True)\n",
    "    rf.fit(x_train, y_train)\n",
    "    scores = rf.cv_results_\n",
    "\n",
    "    for key in parameters.keys():\n",
    "        hyperparameters = key\n",
    "        break\n",
    "\n",
    "    # plotting accuracies for parameters\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_train_score\"], label=\"training accuracy\")\n",
    "    plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_test_score\"], label=\"test accuracy\")\n",
    "    plt.xlabel(hyperparameters)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning max_depth**\n",
    "\n",
    "Let's try to find the optimum values for max_depth and understand how the value of max_depth impacts the overall accuracy of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(2, 30, 5)}\n",
    "tune_hyperparameter(parameters,X_train_pca, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "We can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score becomes stagnant.\n",
    "12 and 18 value have peek convergence and can be used for grid veiw search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to build the model on\n",
    "parameters = {'n_estimators': range(100, 1000, 200)}\n",
    "tune_hyperparameter(parameters,X_train_pca, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "Score almost remain the same with very low dip throught the range. We can use 200 for grid view search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning max_features**\n",
    "\n",
    "Let's find how the model performance varies with max_features, which is the maximum number of features considered for splitting at a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to build the model on\n",
    "parameters = {'max_features': [20,30,40]}\n",
    "tune_hyperparameter(parameters,X_train_pca, y_tr,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Apparently, accuracy of training seems to be stable and test scores seems to decrease after 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(1, 50, 10)}\n",
    "tune_hyperparameter(parameters,X_train_pca, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "\n",
    "10 to 20 seems to be a good range and that will be used in grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(10, 50, 10)}\n",
    "tune_hyperparameter(parameters,X_train_pca, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "\n",
    "Range 10 to 20 is optimal with good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "params = {\n",
    "    'max_depth': [12,18],\n",
    "    'n_estimators': [200],\n",
    "    'max_features': [30],\n",
    "    'min_samples_leaf': [10,20],\n",
    "    'min_samples_split': [10,20]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "# Instantiate the grid search model\n",
    "rf_grid_search = GridSearchCV(estimator = rf, param_grid = params, \n",
    "                          cv = 5, n_jobs = -1,verbose = 1, scoring= \"accuracy\", \n",
    "                          return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search.fit(X_train_pca, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We can get accuracy of {} using \\n{}\".format(round(rf_grid_search.best_score_,2),rf_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and Evaluating the Final Model for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=18,\n",
    "                             max_features=30,\n",
    "                             min_samples_leaf=10,\n",
    "                             min_samples_split=20,\n",
    "                             n_estimators=200,\n",
    "                             n_jobs = -1,random_state=0)\n",
    "#Call model fit with validate set\n",
    "\n",
    "model_fit(rfc,X_train_pca, y_tr, X_test_pca, df_test[response] ,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Iteration 3: Using XGB for Forecasting Model with Feature Importance ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data with default hyperparameters\n",
    "model = XGBClassifier(random_state=0)\n",
    "model_fit(model,x_tr,y_tr, df_test[predictor],df_test[response] ,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning with XGBoost\n",
    "\n",
    "# creating a KFold object \n",
    "folds = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.1,0.2,0.3], \n",
    "             'subsample': [0.3,0.4,0.5]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(random_state=0)\n",
    "\n",
    "#auc scorer\n",
    "scorer = make_scorer(roc_auc_score,\n",
    "                             greater_is_better=True,\n",
    "                             needs_proba=True,\n",
    "                             needs_threshold=False)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= \"accuracy\", # auc score\n",
    "                        cv = folds, \n",
    "                        n_jobs = -1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_cv.fit(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results_xboost = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results_xboost['param_learning_rate'] = cv_results_xboost['param_learning_rate'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy score of **'+str(round(model_cv.best_score_,2))+'** using '+str(model_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_for_xboost(param_grid,cv_results):\n",
    "    plt.figure(figsize=(18,5))\n",
    "    for n, subsample in enumerate(param_grid['subsample']):\n",
    "        # subplot 1/n\n",
    "        plt.subplot(1,len(param_grid['subsample']), n+1)\n",
    "        df = cv_results[cv_results['param_subsample']==subsample]\n",
    "\n",
    "        plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n",
    "        plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n",
    "        plt.xlabel('learning_rate')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(\"subsample={0}\".format(subsample))\n",
    "        plt.ylim([0.60, 1])\n",
    "        plt.legend(['test score', 'train score'], loc='lower right')\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {'learning_rate': [0.1,0.2,0.3], 'subsample': [0.3,0.4,0.5]}  \n",
    "plot_for_xboost(param_grid1,cv_results_xboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Evaluating the Final Model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen hyperparameters\n",
    "# fit model on training data\n",
    "\n",
    "model = XGBClassifier(learning_rate = 0.2, subsample = 0.5, random_state=0)\n",
    "model_fit(model,x_tr,y_tr, df_test[predictor],df_test[response] ,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** - Some features are more important than the others. So definetely  they can contribute towards forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "\n",
    "col = list(X_train.columns)\n",
    "model.get_booster().feature_names = col\n",
    "\n",
    "ax =plot_importance(model.get_booster(),max_num_features=15)\n",
    "figure = ax.figure\n",
    "figure.set_size_inches=(30,45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature is **The total num of times recharge is done for the 8th Month**. This is by far the most important feature\n",
    "\n",
    "The other important features are **Last day recharge Amt** for the 8th month, **Total Recharge Data** for 8th month, **Max Recharge Amt** for the 8th month \n",
    "\n",
    "So in summary, the Recharge value and Frequency of the 8th Month is a significant indicator of churn. This is logical and makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Model Training Summary ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Summary ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Recommendations ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
